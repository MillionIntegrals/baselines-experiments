name: 'qbert_dqn'

env:
  name: vel.rl.env.classic_atari
  game: 'QbertNoFrameskip-v4'


model:
  name: vel.rl.models.q_dueling_model

  backbone:
    name: vel.rl.models.backbone.double_nature_cnn
    input_width: 84
    input_height: 84
    input_channels: 4  # The same as frame_stack


reinforcer:
  name: vel.rl.reinforcers.buffered_single_off_policy_iteration_reinforcer

  algo:
    name: vel.rl.algo.dqn

    double_dqn: true
    target_update_frequency: 10_000  # After how many batches to update the target network
    max_grad_norm: 0.5

  env_roller:
    name: vel.rl.env_roller.single.prioritized_replay_roller_epsgreedy

    buffer_capacity: 250_000
    buffer_initial_size: 30_000
    frame_stack: 4

    priority_exponent: 0.6
    priority_weight:
      name: vel.schedules.linear
      initial_value: 0.4
      final_value: 1.0

    priority_epsilon: 1.0e-6

  epsilon_schedule:
    name: vel.schedules.linear_and_constant
    end_of_interpolation: 0.1
    initial_value: 1.0
    final_value: 0.1

  batch_rollout_rounds: 4 # How many environment steps to perform per batch of training
  batch_size: 32

  discount_factor: 0.99


optimizer:
  name: vel.optimizers.rmsprop
  lr: 2.5e-4
  alpha: 0.95
  momentum: 0.95
  epsilon: 1.0e-1


commands:
  train:
    name: vel.rl.commands.rl_train_command
    total_frames: 1.1e7  # 11M
    batches_per_epoch: 2500

  record:
    name: vel.rl.commands.record_movie_command
    takes: 10
    videoname: 'qbert_vid_{:04}.avi'
    frame_history: 4
    sample_args:
      epsilon: 0.0

  evaluate:
    name: vel.rl.commands.evaluate_env_command
    takes: 100
    frame_history: 4
    sample_args:
      epsilon: 0.0
